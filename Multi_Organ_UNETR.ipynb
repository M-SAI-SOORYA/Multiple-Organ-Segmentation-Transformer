{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "tpu1vmV38",
      "dataSources": [
        {
          "sourceId": 7151372,
          "sourceType": "datasetVersion",
          "datasetId": 4129069
        }
      ],
      "dockerImageVersionId": 30617,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/M-SAI-SOORYA/Multiple-Organ-Segmnetation-Transformer/blob/main/Multi_Organ_UNETR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "RHP-URhWwHC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building The Transformer Model"
      ],
      "metadata": {
        "id": "fgeWNTr7xmTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras.layers as L\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def mlp(x, cf):\n",
        "    x = L.Dense(cf[\"mlp_dim\"], activation=\"gelu\")(x)\n",
        "    x = L.Dropout(cf[\"dropout_rate\"])(x)\n",
        "    x = L.Dense(cf[\"hidden_dim\"])(x)\n",
        "    x = L.Dropout(cf[\"dropout_rate\"])(x)\n",
        "    return x\n",
        "\n",
        "def transformer_encoder(x, cf):\n",
        "    skip_1 = x\n",
        "    x = L.LayerNormalization()(x)\n",
        "    x = L.MultiHeadAttention(\n",
        "        num_heads=cf[\"num_heads\"], key_dim=cf[\"hidden_dim\"]\n",
        "    )(x, x)\n",
        "    x = L.Add()([x, skip_1])\n",
        "\n",
        "    skip_2 = x\n",
        "    x = L.LayerNormalization()(x)\n",
        "    x = mlp(x, cf)\n",
        "    x = L.Add()([x, skip_2])\n",
        "\n",
        "    return x\n",
        "\n",
        "def conv_block(x, num_filters, kernel_size=3):\n",
        "    x = L.Conv2D(num_filters, kernel_size=kernel_size, padding=\"same\")(x)\n",
        "    x = L.BatchNormalization()(x)\n",
        "    x = L.ReLU()(x)\n",
        "    return x\n",
        "\n",
        "def deconv_block(x, num_filters):\n",
        "    x = L.Conv2DTranspose(num_filters, kernel_size=2, padding=\"same\", strides=2)(x)\n",
        "    return x\n",
        "\n",
        "def build_unetr_2d(cf):\n",
        "    \"\"\" Inputs \"\"\"\n",
        "    input_shape = (cf[\"image_size\"], cf[\"image_size\"], cf[\"num_channels\"])\n",
        "    inputs = L.Input(input_shape)\n",
        "\n",
        "    # Resize the input image to the expected patch size\n",
        "    x = L.Reshape((cf[\"num_patches\"], cf[\"patch_size\"] * cf[\"patch_size\"] * cf[\"num_channels\"]))(inputs)\n",
        "    # x = L.Dense(cf[\"hidden_dim\"])(x)\n",
        "\n",
        "    # \"\"\" Inputs \"\"\"\n",
        "    # input_shape = (cf[\"num_patches\"], cf[\"patch_size\"]*cf[\"patch_size\"]*cf[\"num_channels\"])\n",
        "    # inputs = L.Input(input_shape) ## (None, 256, 768)\n",
        "\n",
        "    # \"\"\" Patch + Position Embeddings \"\"\"\n",
        "    patch_embed = L.Dense(cf[\"hidden_dim\"])(x) ## (None, 256, 768)\n",
        "\n",
        "    positions = tf.range(start=0, limit=cf[\"num_patches\"], delta=1) ## (256,)\n",
        "    pos_embed = L.Embedding(input_dim=cf[\"num_patches\"], output_dim=cf[\"hidden_dim\"])(positions) ## (256, 768)\n",
        "    x = patch_embed + pos_embed ## (None, 256, 768)\n",
        "\n",
        "    \"\"\" Transformer Encoder \"\"\"\n",
        "    skip_connection_index = [3, 6, 9, 12]\n",
        "    skip_connections = []\n",
        "\n",
        "    for i in range(1, cf[\"num_layers\"]+1, 1):\n",
        "        x = transformer_encoder(x, cf)\n",
        "\n",
        "        if i in skip_connection_index:\n",
        "            skip_connections.append(x)\n",
        "\n",
        "    \"\"\" CNN Decoder \"\"\"\n",
        "    z3, z6, z9, z12 = skip_connections\n",
        "\n",
        "    ## Reshaping\n",
        "    z0 = L.Reshape((cf[\"image_size\"], cf[\"image_size\"], cf[\"num_channels\"]))(inputs)\n",
        "    z3 = L.Reshape((cf[\"patch_size\"], cf[\"patch_size\"], cf[\"hidden_dim\"]))(z3)\n",
        "    z6 = L.Reshape((cf[\"patch_size\"], cf[\"patch_size\"], cf[\"hidden_dim\"]))(z6)\n",
        "    z9 = L.Reshape((cf[\"patch_size\"], cf[\"patch_size\"], cf[\"hidden_dim\"]))(z9)\n",
        "    z12 = L.Reshape((cf[\"patch_size\"], cf[\"patch_size\"], cf[\"hidden_dim\"]))(z12)\n",
        "\n",
        "    ## Decoder 1\n",
        "    x = deconv_block(z12, 512)\n",
        "\n",
        "    s = deconv_block(z9, 512)\n",
        "    s = conv_block(s, 512)\n",
        "    x = L.Concatenate()([x, s])\n",
        "\n",
        "    x = conv_block(x, 512)\n",
        "    x = conv_block(x, 512)\n",
        "\n",
        "    ## Decoder 2\n",
        "    x = deconv_block(x, 256)\n",
        "\n",
        "    s = deconv_block(z6, 256)\n",
        "    s = conv_block(s, 256)\n",
        "    s = deconv_block(s, 256)\n",
        "    s = conv_block(s, 256)\n",
        "\n",
        "    x = L.Concatenate()([x, s])\n",
        "    x = conv_block(x, 256)\n",
        "    x = conv_block(x, 256)\n",
        "\n",
        "    ## Decoder 3\n",
        "    x = deconv_block(x, 128)\n",
        "\n",
        "    s = deconv_block(z3, 128)\n",
        "    s = conv_block(s, 128)\n",
        "    s = deconv_block(s, 128)\n",
        "    s = conv_block(s, 128)\n",
        "    s = deconv_block(s, 128)\n",
        "    s = conv_block(s, 128)\n",
        "\n",
        "    x = L.Concatenate()([x, s])\n",
        "    x = conv_block(x, 128)\n",
        "    x = conv_block(x, 128)\n",
        "\n",
        "    ## Decoder 4\n",
        "    x = deconv_block(x, 64)\n",
        "\n",
        "    s = conv_block(z0, 64)\n",
        "    s = conv_block(s, 64)\n",
        "\n",
        "    x = L.Concatenate()([x, s])\n",
        "    x = conv_block(x, 64)\n",
        "    x = conv_block(x, 64)\n",
        "\n",
        "    \"\"\" Output \"\"\"\n",
        "    # outputs = L.Conv2D(11, kernel_size=1, padding=\"same\", activation=\"sigmoid\")(x)\n",
        "    outputs = L.Conv2D(11, kernel_size=1, padding=\"same\", activation=\"softmax\")(x)\n",
        "\n",
        "\n",
        "    return Model(inputs, outputs, name=\"UNETR_2D\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    config = {}\n",
        "    config[\"image_size\"] = 256\n",
        "    config[\"num_layers\"] = 12\n",
        "    config[\"hidden_dim\"] = 768\n",
        "    config[\"mlp_dim\"] = 3072\n",
        "    config[\"num_heads\"] = 12\n",
        "    config[\"dropout_rate\"] = 0.1\n",
        "    config[\"num_patches\"] = 256\n",
        "    config[\"patch_size\"] = 16\n",
        "    config[\"num_channels\"] = 3\n",
        "\n",
        "    model = build_unetr_2d(config)\n",
        "    model.summary()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-08T04:16:34.46433Z",
          "iopub.execute_input": "2023-12-08T04:16:34.464551Z",
          "iopub.status.idle": "2023-12-08T04:16:56.482778Z",
          "shell.execute_reply.started": "2023-12-08T04:16:34.464525Z",
          "shell.execute_reply": "2023-12-08T04:16:56.481867Z"
        },
        "trusted": true,
        "id": "UgJeYNaJwHC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Model"
      ],
      "metadata": {
        "id": "7HjBMVdexq90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from glob import glob\n",
        "import scipy.io\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger, EarlyStopping\n",
        "\n",
        "\"\"\" Global parameters \"\"\"\n",
        "global IMG_H\n",
        "global IMG_W\n",
        "global NUM_CLASSES\n",
        "global CLASSES\n",
        "global COLORMAP\n",
        "\n",
        "\"\"\" Creating a directory \"\"\"\n",
        "def create_dir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "\"\"\" Load and split the dataset \"\"\"\n",
        "def load_dataset(path, split=0.2):\n",
        "    images = sorted(glob(os.path.join(path, \"Training\", \"img_folder\", \"*\")))[:5000]\n",
        "    masks = sorted(glob(os.path.join(path, \"Training\", \"mask_folder\", \"*\")))[:5000]\n",
        "\n",
        "    split_size = int(split * len(images))\n",
        "\n",
        "    train_x, valid_x = train_test_split(images, test_size=split_size, random_state=42)\n",
        "    train_y, valid_y = train_test_split(masks, test_size=split_size, random_state=42)\n",
        "\n",
        "    train_x, test_x = train_test_split(train_x, test_size=split_size, random_state=42)\n",
        "    train_y, test_y = train_test_split(train_y, test_size=split_size, random_state=42)\n",
        "\n",
        "    return (train_x, train_y), (valid_x, valid_y), (test_x, test_y)\n",
        "\n",
        "\n",
        "def get_colormap(path):\n",
        "    mat_path = os.path.join(path, \"ultimate.mat\")\n",
        "    colormap = scipy.io.loadmat(mat_path)[\"color\"]\n",
        "\n",
        "    classes = [\n",
        "        \"Background\",\n",
        "        \"Spleen\",\n",
        "        \"Right kidney\",\n",
        "        \"Left kidney\",\n",
        "        \"Gallbladder\",\n",
        "        \"Liver\",\n",
        "        \"Stomach\",\n",
        "        \"Aorta\",\n",
        "        \"Inferior vena cava\",\n",
        "        \"Portal vein\",\n",
        "        \"Pancreas\"\n",
        "    ]\n",
        "\n",
        "    return classes, colormap\n",
        "\n",
        "def read_image(x):\n",
        "    x = cv2.imread(x, cv2.IMREAD_COLOR)\n",
        "    x = cv2.resize(x, (IMG_W, IMG_H))\n",
        "    x = x.astype(np.float32)\n",
        "    return x\n",
        "\n",
        "def read_mask(x):\n",
        "    x = cv2.imread(x, cv2.IMREAD_COLOR)\n",
        "    x = cv2.resize(x, (IMG_W, IMG_H))\n",
        "\n",
        "    output = []\n",
        "\n",
        "    for color in COLORMAP:\n",
        "        cmap = np.all(np.equal(x, color), axis=-1)\n",
        "        output.append(cmap)\n",
        "\n",
        "    output = np.stack(output, axis=-1)\n",
        "    output = output.astype(np.uint8)\n",
        "\n",
        "    return output\n",
        "\n",
        "def preprocess(x, y):\n",
        "    def f(x, y):\n",
        "        x = x.decode()\n",
        "        y = y.decode()\n",
        "\n",
        "        x = read_image(x)\n",
        "        y = read_mask(y)\n",
        "\n",
        "        return x, y\n",
        "\n",
        "    image, mask = tf.numpy_function(f, [x, y], [tf.float32, tf.uint8])\n",
        "    image.set_shape([IMG_H, IMG_W, 3])\n",
        "    mask.set_shape([IMG_H, IMG_W, NUM_CLASSES])\n",
        "\n",
        "    return image, mask\n",
        "\n",
        "def tf_dataset(x, y, batch=8):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
        "    dataset = dataset.shuffle(buffer_size=5000)\n",
        "    dataset = dataset.map(preprocess)\n",
        "    dataset = dataset.batch(batch)\n",
        "    dataset = dataset.prefetch(2)\n",
        "    return dataset\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \"\"\" Seeding \"\"\"\n",
        "    np.random.seed(42)\n",
        "    tf.random.set_seed(42)\n",
        "\n",
        "    \"\"\" Directory for storing files \"\"\"\n",
        "    create_dir(\"files\")\n",
        "\n",
        "    \"\"\" Hyperparameters \"\"\"\n",
        "    IMG_H = 256\n",
        "    IMG_W = 256\n",
        "    NUM_CLASSES = 11\n",
        "\n",
        "    batch_size = 16  # Adjust this based on TPU memory constraints\n",
        "    lr = 1e-4\n",
        "    num_epochs = 60\n",
        "\n",
        "    config = {\n",
        "        \"image_size\": 256,\n",
        "        \"num_layers\": 12,\n",
        "        \"hidden_dim\": 768,\n",
        "        \"mlp_dim\": 3072,\n",
        "        \"num_heads\": 12,\n",
        "        \"dropout_rate\": 0.1,\n",
        "        \"num_patches\": 256,\n",
        "        \"patch_size\": 16,\n",
        "        \"num_channels\": 3\n",
        "    }\n",
        "\n",
        "    dataset_path = \"/kaggle/input/ultimate-data/Ultimate_dataset-20231206T103555Z-001/Ultimate_dataset\"\n",
        "    model_path = os.path.join(\"files\", \"Ultimate_unter_model1.h5\")\n",
        "    csv_path = os.path.join(\"files\", \"Unetr_Ultimate_data.csv\")\n",
        "\n",
        "    \"\"\" Loading the dataset \"\"\"\n",
        "    (train_x, train_y), (valid_x, valid_y), (test_x, test_y) = load_dataset(dataset_path)\n",
        "    print(f\"Train: {len(train_x)}/{len(train_y)} - Valid: {len(valid_x)}/{len(valid_y)} - Test: {len(test_x)}/{len(test_x)}\")\n",
        "\n",
        "    \"\"\" Process the colormap \"\"\"\n",
        "    CLASSES, COLORMAP = get_colormap(dataset_path)\n",
        "\n",
        "    \"\"\" Dataset Pipeline \"\"\"\n",
        "    train_dataset = tf_dataset(train_x, train_y, batch=batch_size)\n",
        "    valid_dataset = tf_dataset(valid_x, valid_y, batch=batch_size)\n",
        "\n",
        "    \"\"\" TPU Configuration \"\"\"\n",
        "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    tf.config.experimental_connect_to_cluster(resolver)\n",
        "    tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
        "\n",
        "    with strategy.scope():\n",
        "        \"\"\" Model \"\"\"\n",
        "        model = build_unetr_2d(config)\n",
        "        model.compile(\n",
        "            loss=\"categorical_crossentropy\",\n",
        "            optimizer=tf.keras.optimizers.Adam(lr)\n",
        "        )\n",
        "\n",
        "    \"\"\" Training \"\"\"\n",
        "    callbacks = [\n",
        "        ModelCheckpoint(model_path, verbose=1, save_best_only=True),\n",
        "        ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-7, verbose=1),\n",
        "        CSVLogger(csv_path, append=True),\n",
        "        EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=False)\n",
        "    ]\n",
        "\n",
        "    model.fit(train_dataset,\n",
        "              validation_data=valid_dataset,\n",
        "              epochs=num_epochs,\n",
        "              callbacks=callbacks\n",
        "              )"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-08T04:19:14.558738Z",
          "iopub.execute_input": "2023-12-08T04:19:14.559097Z",
          "iopub.status.idle": "2023-12-08T07:13:32.882424Z",
          "shell.execute_reply.started": "2023-12-08T04:19:14.559068Z",
          "shell.execute_reply": "2023-12-08T07:13:32.881425Z"
        },
        "trusted": true,
        "id": "KTs8nKekwHC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing The Model"
      ],
      "metadata": {
        "id": "uSUJh_iEwHC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import pandas as pd\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import accuracy_score, f1_score, jaccard_score, precision_score, recall_score\n",
        "import scipy.io\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "# from train import load_dataset, create_dir, get_colormap\n",
        "\n",
        "\"\"\" Global parameters \"\"\"\n",
        "global IMG_H\n",
        "global IMG_W\n",
        "global NUM_CLASSES\n",
        "global CLASSES\n",
        "global COLORMAP\n",
        "\n",
        "\"\"\" Creating a directory \"\"\"\n",
        "def create_dir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "\"\"\" Load and split the dataset \"\"\"\n",
        "def load_dataset(path, split=0.2):\n",
        "    images = sorted(glob(os.path.join(path, \"Training\", \"img_folder\", \"*\")))[:5000]\n",
        "    masks = sorted(glob(os.path.join(path, \"Training\", \"mask_folder\", \"*\")))[:5000]\n",
        "\n",
        "    split_size = int(split * len(images))\n",
        "\n",
        "    train_x, valid_x = train_test_split(images, test_size=split_size, random_state=42)\n",
        "    train_y, valid_y = train_test_split(masks, test_size=split_size, random_state=42)\n",
        "\n",
        "    train_x, test_x = train_test_split(train_x, test_size=split_size, random_state=42)\n",
        "    train_y, test_y = train_test_split(train_y, test_size=split_size, random_state=42)\n",
        "\n",
        "    return (train_x, train_y), (valid_x, valid_y), (test_x, test_y)\n",
        "\n",
        "\n",
        "def get_colormap(path):\n",
        "    mat_path = os.path.join(path, \"ultimate.mat\")\n",
        "    colormap = scipy.io.loadmat(mat_path)[\"color\"]\n",
        "\n",
        "    classes = [\n",
        "        \"Background\",\n",
        "        \"Spleen\",\n",
        "        \"Right kidney\",\n",
        "        \"Left kidney\",\n",
        "        \"Gallbladder\",\n",
        "        \"Liver\",\n",
        "        \"Stomach\",\n",
        "        \"Aorta\",\n",
        "        \"Inferior vena cava\",\n",
        "        \"Portal vein\",\n",
        "        \"Pancreas\"\n",
        "    ]\n",
        "\n",
        "    return classes, colormap\n",
        "\n",
        "def read_image(x):\n",
        "    x = cv2.imread(x, cv2.IMREAD_COLOR)\n",
        "    x = cv2.resize(x, (IMG_W, IMG_H))\n",
        "    x = x.astype(np.float32)\n",
        "    return x\n",
        "\n",
        "def read_mask(x):\n",
        "    x = cv2.imread(x, cv2.IMREAD_COLOR)\n",
        "    x = cv2.resize(x, (IMG_W, IMG_H))\n",
        "\n",
        "    #Masl processing\n",
        "    output=[]\n",
        "    # for i,color in enumerate(COLORMAP):\n",
        "    #   cmap = np.all(np.equal(x, color), axis=-1)\n",
        "    #   cv2.imwrite(f\"cmap{i}.png\", cmap*255)\n",
        "    for color in COLORMAP:\n",
        "      cmap = np.all(np.equal(x, color), axis=-1)\n",
        "      output.append(cmap)\n",
        "\n",
        "\n",
        "    output = np.stack(output, axis=-1)\n",
        "    output = output.astype(np.uint8)\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "def preprocess(x, y):\n",
        "    def f(x, y):\n",
        "        x = x.decode()\n",
        "        y = y.decode()\n",
        "\n",
        "        x =read_image(x)\n",
        "        y=read_mask(y)\n",
        "\n",
        "        return x,y\n",
        "\n",
        "    image, mask = tf.numpy_function(f, [x, y], [tf.float32, tf.uint8])\n",
        "    image.set_shape([IMG_H, IMG_W, 3])\n",
        "    mask.set_shape([IMG_H, IMG_W, NUM_CLASSES])\n",
        "\n",
        "    return image, mask\n",
        "\n",
        "def tf_dataset(x, y, batch=8):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
        "    dataset = dataset.shuffle(buffer_size=5000)\n",
        "    dataset = dataset.map(preprocess)\n",
        "    dataset = dataset.batch(batch)\n",
        "    dataset = dataset.prefetch(2)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def grayscale_to_rgb(mask, classes, colormap):\n",
        "    h, w, _ = mask.shape\n",
        "    mask = mask.astype(np.int32)\n",
        "    output = []\n",
        "\n",
        "    for i, pixel in enumerate(mask.flatten()):\n",
        "        output.append(colormap[pixel])\n",
        "\n",
        "    output = np.reshape(output, (h, w, 3))\n",
        "    return output\n",
        "\n",
        "def save_results(image, mask, pred, save_image_path):\n",
        "    # print(image.shape,mask.shape,pred.shape)\n",
        "    h, w, _ = image.shape\n",
        "    line = np.ones((h, 10, 3)) * 255\n",
        "\n",
        "    pred = np.expand_dims(pred, axis=-1)\n",
        "    pred = grayscale_to_rgb(pred, CLASSES, COLORMAP)\n",
        "\n",
        "    # Ensure both images have the same shape\n",
        "    assert image.shape == mask.shape\n",
        "    assert image.shape == pred.shape\n",
        "\n",
        "    # Blend the images using the alpha parameter\n",
        "    alpha = 0.5\n",
        "    blended_image1 = alpha * image + (1 - alpha) * mask\n",
        "    blended_image2 = alpha * image + (1 - alpha) * pred\n",
        "    # alpha = 0.5\n",
        "    # blended_image1 = Image.blend(image, mask, alpha)\n",
        "    # blended_image2 = Image.blend(image, pred, alpha)\n",
        "\n",
        "    cat_images = np.concatenate([image, line, mask, line, pred, line, blended_image1, blended_image2], axis=1)\n",
        "    cv2.imwrite(save_image_path, cat_images)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \"\"\" Seeding \"\"\"\n",
        "    np.random.seed(42)\n",
        "    tf.random.set_seed(42)\n",
        "\n",
        "    \"\"\" Directory for storing files \"\"\"\n",
        "    create_dir(\"files\")\n",
        "\n",
        "    \"\"\" Directory for storing files \"\"\"\n",
        "    create_dir(\"results2/predictions\")\n",
        "\n",
        "    \"\"\" Hyperparameters \"\"\"\n",
        "    IMG_H = 256\n",
        "    IMG_W = 256\n",
        "    NUM_CLASSES = 11\n",
        "    dataset_path = \"/kaggle/input/ultimate-data/Ultimate_dataset-20231206T103555Z-001/Ultimate_dataset\"\n",
        "    # dataset_path = \"/content/drive/MyDrive/human\"\n",
        "    # model_path = os.path.join(\"files\", \"model1.h5\")\n",
        "    model_path = \"/kaggle/working/files/Ultimate_unter_model1.h5\"\n",
        "\n",
        "    \"\"\" Colormap \"\"\"\n",
        "    CLASSES, COLORMAP = get_colormap(dataset_path)\n",
        "\n",
        "    \"\"\" Model \"\"\"\n",
        "    model = tf.keras.models.load_model(model_path)\n",
        "    # model.summary()\n",
        "\n",
        "    \"\"\" Load the dataset \"\"\"\n",
        "    (train_x, train_y), (valid_x, valid_y), (test_x, test_y) = load_dataset(dataset_path)\n",
        "    print(f\"Train: {len(train_x)}/{len(train_y)} - Valid: {len(valid_x)}/{len(valid_y)} - Test: {len(test_x)}/{len(test_y)}\")\n",
        "    print(\"\")\n",
        "\n",
        "    # Prediction and Evaluation\n",
        "\n",
        "    SCORE = []\n",
        "    for x, y in tqdm(zip(test_x, test_y), total=len(test_x)):\n",
        "      name = x.split(\"/\")[-1].split(\".\")[0]\n",
        "\n",
        "      \"\"\" Reading the image \"\"\"\n",
        "      image = cv2.imread(x, cv2.IMREAD_COLOR)\n",
        "      image = cv2.resize(image, (IMG_W, IMG_H))\n",
        "      image_x = image\n",
        "      # image = image/255.0\n",
        "      image = np.expand_dims(image, axis=0)\n",
        "\n",
        "      \"\"\" Reading the mask \"\"\"\n",
        "      mask = cv2.imread(y, cv2.IMREAD_COLOR)\n",
        "      mask = cv2.resize(mask, (IMG_W, IMG_H))\n",
        "      mask_x = mask\n",
        "      onehot_mask = []\n",
        "      for color in COLORMAP:\n",
        "          cmap = np.all(np.equal(mask, color), axis=-1)\n",
        "          onehot_mask.append(cmap)\n",
        "      onehot_mask = np.stack(onehot_mask, axis=-1)\n",
        "      onehot_mask = np.argmax(onehot_mask, axis=-1)\n",
        "      onehot_mask = onehot_mask.astype(np.int32)\n",
        "\n",
        "      \"\"\" Prediction \"\"\"\n",
        "      pred = model.predict(image, verbose=0)[0]\n",
        "      pred = np.argmax(pred, axis=-1)\n",
        "      pred = pred.astype(np.float32)\n",
        "\n",
        "      \"\"\" Saving the prediction \"\"\"\n",
        "      save_image_path = f\"results2/predictions/{name}.png\"\n",
        "      save_results(image_x, mask_x, pred, save_image_path)\n",
        "\n",
        "      \"\"\" Flatten the array \"\"\"\n",
        "      onehot_mask = onehot_mask.flatten()\n",
        "      pred = pred.flatten()\n",
        "\n",
        "      labels = [i for i in range(NUM_CLASSES)]\n",
        "\n",
        "      \"\"\" Calculating the metrics values \"\"\"\n",
        "      f1_value = f1_score(onehot_mask, pred, labels=labels, average=None, zero_division=0)\n",
        "      jac_value = jaccard_score(onehot_mask, pred, labels=labels, average=None, zero_division=0)\n",
        "\n",
        "      SCORE.append([f1_value, jac_value])\n",
        "\n",
        "    \"\"\" Metrics values \"\"\"\n",
        "    score = np.array(SCORE)\n",
        "    score = np.mean(score, axis=0)\n",
        "\n",
        "    # Calculate accuracy using true labels and predicted labels\n",
        "    accuracy = accuracy_score(onehot_mask, pred.flatten())\n",
        "\n",
        "    f = open(\"files/score.csv\", \"w\")\n",
        "    f.write(\"Class,F1,Jaccard,Accuracy\\n\")\n",
        "\n",
        "    l = [\"Class\", \"F1\", \"Jaccard\", \"Accuracy\"]\n",
        "    print(f\"{l[0]:15s} {l[1]:10s} {l[2]:10s} {l[3]:10s}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    for i in range(score.shape[1]):\n",
        "        class_name = CLASSES[i]\n",
        "        f1 = score[0, i]\n",
        "        jac = score[1, i]\n",
        "        dstr = f\"{class_name:15s}: {f1:1.5f} - {jac:1.5f} - {accuracy:1.5f}\"\n",
        "        print(dstr)\n",
        "        f.write(f\"{class_name:15s},{f1:1.5f},{jac:1.5f},{accuracy:1.5f}\\n\")\n",
        "\n",
        "    print(\"-\" * 50)\n",
        "    class_mean = np.mean(score, axis=-1)\n",
        "    class_name = \"Mean\"\n",
        "    f1 = class_mean[0]\n",
        "    jac = class_mean[1]\n",
        "    dstr = f\"{class_name:15s}: {f1:1.5f} - {jac:1.5f} - {accuracy:1.5f}\"\n",
        "    print(dstr)\n",
        "    f.write(f\"{class_name:15s},{f1:1.5f},{jac:1.5f},{accuracy:1.5f}\\n\")\n",
        "\n",
        "    f.close()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-08T07:18:05.344608Z",
          "iopub.execute_input": "2023-12-08T07:18:05.344974Z",
          "iopub.status.idle": "2023-12-08T07:31:10.206738Z",
          "shell.execute_reply.started": "2023-12-08T07:18:05.344943Z",
          "shell.execute_reply": "2023-12-08T07:31:10.205613Z"
        },
        "trusted": true,
        "id": "LlH1xoniwHC6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}